# -*- coding: utf-8 -*-
"""Lenet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pIpRVGw31Z2Fb0D-8j7qX6gbegbXoRGA
"""

from keras.datasets import mnist

from keras.optimizers import SGD
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.convolutional import AveragePooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dense

import numpy as np
import cv2
import matplotlib.pyplot as plt

from google.colab.patches import cv2_imshow

def build_lenet(width,height,depth,classes):
  model= Sequential()
  model.add(Conv2D(6,(5, 5),padding="valid",input_shape=(height,width,depth)))
  model.add(Activation("tanh"))
  model.add(AveragePooling2D(pool_size=(2,2),strides=(2, 2)))

  model.add(Conv2D(16,(5, 5),padding="valid"))
  model.add(Activation("tanh"))
  model.add(AveragePooling2D(pool_size=(2,2),strides=(2,2)))

  model.add(Conv2D(120,(5, 5),padding="valid"))
  model.add(Activation("tanh"))

  model.add(Flatten())
  model.add(Dense(84))
  model.add(Activation("tanh"))

  model.add(Dense(classes))
  model.add(Activation("softmax"))

  return model

def graph_history_training(history):
  plt.rcParams["figure.figsize"] = (12,9)
  plt.style.use('ggplot')
  plt.figure(1)

  plt.subplot(211)
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title("Model Accuracy")
  plt.ylabel("Accuracy")
  plt.xlabel("Epoch")
  plt.legend(['Training','Validation'],loc='lower right')

  plt.subplot(212)
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title("Model Loss")
  plt.ylabel("Loss")
  plt.xlabel("Epoch")
  plt.legend(['Training','Validation'],loc='upper right')

  plt.tight_layout()
  plt.show()

(trainData,trainLabels),(testData,testLabels) = mnist.load_data()

print(trainData.shape,testData.shape)

trainX=np.zeros((60000,32,32))
testX=np.zeros((10000,32,32))
for i in range(trainData.shape[0]):
  padded_array = np.pad(trainData[i], 2, mode='constant')
  trainX[i]=padded_array
for i in range(testData.shape[0]):
  padded_array = np.pad(testData[i], 2, mode='constant')
  testX[i]=padded_array

import sys
np.set_printoptions(threshold=sys.maxsize)
print(trainX.shape,testX.shape,trainLabels.shape,testLabels.shape)
print(trainLabels[0])

trainX = trainX[:,:,:,np.newaxis]
testX = testX[:,:,:,np.newaxis]

trainX = trainX/255.0
testX = testX/255.0

trainLabels = np_utils.to_categorical(trainLabels,10)
testLabels = np_utils.to_categorical(testLabels,10)

opt= SGD(lr=0.01)
model = build_lenet(width=32,height=32,depth=1,classes=10)
print(model.summary())

model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=["accuracy"])

history = model.fit(trainX,trainLabels,batch_size=128,epochs=60,validation_split=0.1,verbose=1)

(val_loss,val_accuracy)= model.evaluate(testX,testLabels,batch_size=128,verbose=1)
print(val_accuracy*100)
print(val_loss)

graph_history_training(history)

for i in np.random.choice(np.arange(0,len(testLabels)),size=(10,)):
  probs=model.predict(testX[np.newaxis,1])
  prediction = probs.argmax(axis=1)

  image = (testX[i]*255).astype("uint8")
  image=cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)

  image = cv2.resize(image,(280,280),interpolation = cv2.INTER_LINEAR)

  cv2.putText(image,str(prediction[0]),(20,40),cv2.FONT_HERSHEY_DUPLEX,1.5,(0,255,0),1)

  print("\n")

  print("Predicted: {}, Actual: {}".format(prediction[0],np.argmax(testLabels[i])))

  cv2_imshow(image)

